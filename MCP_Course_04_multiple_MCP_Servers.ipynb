{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBVmoiYas0T51FXM1u1u20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farhaj499/MCP_Model_Context_Protocol_Course/blob/main/MCP_Course_04_multiple_MCP_Servers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 6: Connecting the MCP Chatbot to Reference Servers"
      ],
      "metadata": {
        "id": "VbPjJRuYY1Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open-Source MCP Servers"
      ],
      "metadata": {
        "id": "URB_OvsjY-jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this [repo](https://github.com/modelcontextprotocol/servers), you can find a collection of reference implementations for the MCP servers, as well as references to community built servers and additional resources. You will use two of the reference servers to integrate their tools in your MCP chatbot:\n",
        "- [fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch): provides the `fetch` tool which fetches a URL from the internet and extracts its contents as markdown.\n",
        "- [filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem): provides several tools for interacting with the files and directories within a directory that you specify.\n",
        "\n",
        "You can check the readme file of each server to check the features they expose and how to run them.  "
      ],
      "metadata": {
        "id": "5YYNQUukZB2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the updates you'll make to the chatbot. You're encouraged to read this section before or after you watch the video, if you'd like to learn more about the details of the code.\n",
        "\n",
        "- Instead of hardcoding the server parameters in the chatbot, the chatbot will read the server configurations from a JSON file:\n",
        "  ### Server Configuration\n",
        "  In the `L6/mcp_project`, you can find the `server_config.json` configuration file that has the following structure.\n",
        "    ``` json\n",
        "    {\n",
        "        \"mcpServers\": {\n",
        "            \n",
        "            \"filesystem\": {\n",
        "                \"command\": \"npx\",\n",
        "                \"args\": [\n",
        "                    \"-y\",\n",
        "                    \"@modelcontextprotocol/server-filesystem\",\n",
        "                    \".\"\n",
        "                ]\n",
        "            },\n",
        "            \n",
        "            \"research\": {\n",
        "                \"command\": \"uv\",\n",
        "                \"args\": [\"run\", \"research_server.py\"]\n",
        "            },\n",
        "            \n",
        "            \"fetch\": {\n",
        "                \"command\": \"uvx\",\n",
        "                \"args\": [\"mcp-server-fetch\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    ```\n",
        "    For the reference servers, the commands `npx` and `uvx` directly install the files of the servers to your local environment (you don't need to install them ahead of time). Note for the `filesystem`, the `.` is provided as the third argument and it means \"current directory\". This means that you're allowing for the `fetch` server to interact with the files and directories that are within the current directory."
      ],
      "metadata": {
        "id": "AO2ck4LFj9Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here's a rough sketch of the diagram of the updated MCP_chatbot:\n",
        "  \n",
        "  <img src=\"images/updated_class.png\" width=\"600\">\n",
        "\n",
        "  1. Instead of having one session, you now have a list of client sessions where each client session establishes a 1-to-1 connection to each server;\n",
        "  2. `available_tools` includes the definitions of all the tools exposed by all servers that the chatbot can connect to.\n",
        "  3. `tool_to_session` maps the tool name to the corresponding client session; in this way, when the LLM decides on a particular tool name, you can map it to the correct client session so you can use that session to send `tool_call` request to the right MCP server.\n",
        "  4. `exit_stack` is a context manager that will manage the mcp client objects and their sessions and ensures that they are properly closed. In lesson 5, you did not use it because you used the `with` statement which behind the scenes uses a context manager. Here you could again use the `with` statement, but you may end up using multiple nested `with` statements since you have multiple servers to connect to. `exit_stack` allows you to dynamically add the mcp clients and their sessions as you'll see in the code below.\n",
        "  5. `connect_to_servers` reads the server configuration file and for each single server, it calls the helper method `connect_to_server`. In this latter method, an MCP client is created and used to launch the server as a sub-process and then a client session is created to connect to the server and get a description of the list of the tools provided by the server.\n",
        "  6. `cleanup` is a helper method that ensures all your connections are properly shut down when you're done with them. In lesson 5, you relied on the `with` statement to automatically clean up resources. This cleanup method serves a similar purpose, but for all the resources you've added to your exit_stack; it closes (your MCP clients and sessions) in the reverse order they were added - like stacking and unstacking plates. This is particularly important in network programming to avoid resource leaks."
      ],
      "metadata": {
        "id": "gYA4W5z5kCxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCP Chatbot"
      ],
      "metadata": {
        "id": "YJt_1MFdkFSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mcp_project/mcp_chatbot.py\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from anthropic import Anthropic\n",
        "from mcp import ClientSession, StdioServerParameters, types\n",
        "from mcp.client.stdio import stdio_client\n",
        "from typing import List, Dict, TypedDict\n",
        "from contextlib import AsyncExitStack\n",
        "import json\n",
        "import asyncio\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class ToolDefinition(TypedDict):\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: dict\n",
        "\n",
        "class MCP_ChatBot:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize session and client objects\n",
        "        self.sessions: List[ClientSession] = [] # new\n",
        "        self.exit_stack = AsyncExitStack() # new\n",
        "        self.anthropic = Anthropic()\n",
        "        self.available_tools: List[ToolDefinition] = [] # new\n",
        "        self.tool_to_session: Dict[str, ClientSession] = {} # new\n",
        "\n",
        "\n",
        "    async def connect_to_server(self, server_name: str, server_config: dict) -> None:\n",
        "        \"\"\"Connect to a single MCP server.\"\"\"\n",
        "        try:\n",
        "            server_params = StdioServerParameters(**server_config)\n",
        "            stdio_transport = await self.exit_stack.enter_async_context(\n",
        "                stdio_client(server_params)\n",
        "            ) # new\n",
        "            read, write = stdio_transport\n",
        "            session = await self.exit_stack.enter_async_context(\n",
        "                ClientSession(read, write)\n",
        "            ) # new\n",
        "            await session.initialize()\n",
        "            self.sessions.append(session)\n",
        "\n",
        "            # List available tools for this session\n",
        "            response = await session.list_tools()\n",
        "            tools = response.tools\n",
        "            print(f\"\\nConnected to {server_name} with tools:\", [t.name for t in tools])\n",
        "\n",
        "            for tool in tools: # new\n",
        "                self.tool_to_session[tool.name] = session\n",
        "                self.available_tools.append({\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"input_schema\": tool.inputSchema\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to connect to {server_name}: {e}\")\n",
        "\n",
        "    async def connect_to_servers(self): # new\n",
        "        \"\"\"Connect to all configured MCP servers.\"\"\"\n",
        "        try:\n",
        "            with open(\"server_config.json\", \"r\") as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "            servers = data.get(\"mcpServers\", {})\n",
        "\n",
        "            for server_name, server_config in servers.items():\n",
        "                await self.connect_to_server(server_name, server_config)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading server configuration: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def process_query(self, query):\n",
        "        messages = [{'role':'user', 'content':query}]\n",
        "        response = self.anthropic.messages.create(max_tokens = 2024,\n",
        "                                      model = 'claude-3-7-sonnet-20250219',\n",
        "                                      tools = self.available_tools,\n",
        "                                      messages = messages)\n",
        "        process_query = True\n",
        "        while process_query:\n",
        "            assistant_content = []\n",
        "            for content in response.content:\n",
        "                if content.type =='text':\n",
        "                    print(content.text)\n",
        "                    assistant_content.append(content)\n",
        "                    if(len(response.content) == 1):\n",
        "                        process_query= False\n",
        "                elif content.type == 'tool_use':\n",
        "                    assistant_content.append(content)\n",
        "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
        "                    tool_id = content.id\n",
        "                    tool_args = content.input\n",
        "                    tool_name = content.name\n",
        "\n",
        "\n",
        "                    print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
        "\n",
        "                    # Call a tool\n",
        "                    session = self.tool_to_session[tool_name] # new\n",
        "                    result = await session.call_tool(tool_name, arguments=tool_args)\n",
        "                    messages.append({\"role\": \"user\",\n",
        "                                      \"content\": [\n",
        "                                          {\n",
        "                                              \"type\": \"tool_result\",\n",
        "                                              \"tool_use_id\":tool_id,\n",
        "                                              \"content\": result.content\n",
        "                                          }\n",
        "                                      ]\n",
        "                                    })\n",
        "                    response = self.anthropic.messages.create(max_tokens = 2024,\n",
        "                                      model = 'claude-3-7-sonnet-20250219',\n",
        "                                      tools = self.available_tools,\n",
        "                                      messages = messages)\n",
        "\n",
        "                    if(len(response.content) == 1 and response.content[0].type == \"text\"):\n",
        "                        print(response.content[0].text)\n",
        "                        process_query= False\n",
        "\n",
        "\n",
        "\n",
        "    async def chat_loop(self):\n",
        "        \"\"\"Run an interactive chat loop\"\"\"\n",
        "        print(\"\\nMCP Chatbot Started!\")\n",
        "        print(\"Type your queries or 'quit' to exit.\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\nQuery: \").strip()\n",
        "\n",
        "                if query.lower() == 'quit':\n",
        "                    break\n",
        "\n",
        "                await self.process_query(query)\n",
        "                print(\"\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError: {str(e)}\")\n",
        "\n",
        "    async def cleanup(self): # new\n",
        "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
        "        await self.exit_stack.aclose()\n",
        "\n",
        "\n",
        "async def main():\n",
        "    chatbot = MCP_ChatBot()\n",
        "    try:\n",
        "        # the mcp clients and sessions are not initialized using \"with\"\n",
        "        # like in the previous lesson\n",
        "        # so the cleanup should be manually handled\n",
        "        await chatbot.connect_to_servers() # new!\n",
        "        await chatbot.chat_loop()\n",
        "    finally:\n",
        "        await chatbot.cleanup() #new!\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "9qaf6UHvkIQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the MCP Chatbot\n",
        "**Terminal Instructions**\n",
        "- To open the terminal, run the cell below.\n",
        "- Navigate to the `mcp_project` directory:\n",
        "    - `cd L6/mcp_project`\n",
        "- Activate the virtual environment:\n",
        "    - `source .venv/bin/activate`\n",
        "- Run the chatbot:\n",
        "    - `uv run mcp_chatbot.py`\n",
        "- To exit the chatbot, type `quit`.\n",
        "- If you run some queries and would like to access the `papers` folder or any output files: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L6` -> `mcp_project`."
      ],
      "metadata": {
        "id": "hTh6G5DJkMoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start a new terminal\n",
        "import os\n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(f\"{os.environ.get('DLAI_LOCAL_URL').format(port=8888)}terminals/3\", width=600, height=768)"
      ],
      "metadata": {
        "id": "3E4NWMqekRAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}